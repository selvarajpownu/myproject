{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac8a282",
   "metadata": {},
   "source": [
    "# Build the model for questionanswering transformer from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a130eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af96fe71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd25bac2e8964f00b9338cb21f177b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852cd1143540474ab38d4ef22d8d8a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956ecbef4b4844929c1094a7cff4d8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8c9d4f70144df284af150d47445c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954eb186bdbe4415aca4cf29a25bface",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1743e34b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = datasets.load_dataset(\"squad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca60fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661181',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'What is the Grotto at Notre Dame?',\n",
       " 'answers': {'text': ['a Marian place of prayer and reflection'],\n",
       "  'answer_start': [381]}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf8c6c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (87599, 5), 'validation': (10570, 5)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e017d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller splits for training and validation\n",
    "train_dataset = dataset[\"train\"].select(list(range(1000)))\n",
    "valid_dataset = dataset[\"train\"].select(list(range(1000, 1000 + 200)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d5a03dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba12c96d09e4d65a07dc6c6a0c07357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c9609afa77443d88cb030bbf98a591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the prepare_data function\n",
    "def prepare_data(example, tokenizer):\n",
    "    context = example[\"context\"]\n",
    "    question = example[\"question\"]\n",
    "    encoding = tokenizer(context, question, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    return encoding\n",
    "\n",
    "# Convert the datasets to a format that the model can understand\n",
    "encoded_train_dataset = train_dataset.map(lambda example: prepare_data(example, tokenizer), batched=True)\n",
    "encoded_valid_dataset = valid_dataset.map(lambda example: prepare_data(example, tokenizer), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1677f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 03:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_valid_dataset,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd16cb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 257.8676, 'eval_samples_per_second': 0.776, 'eval_steps_per_second': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation results\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d25b0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to answer questions\n",
    "def answer_question(context, question, tokenizer):\n",
    "    encoding = tokenizer(context, question, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(**encoding)\n",
    "    \n",
    "    start_logits = outputs.start_logits.squeeze()\n",
    "    end_logits = outputs.end_logits.squeeze()\n",
    "\n",
    "    top_start_index = torch.argmax(start_logits)\n",
    "    top_end_index = torch.argmax(end_logits) + 1\n",
    "\n",
    "    # Check if the model did not find a valid answer\n",
    "    if top_start_index >= top_end_index or top_end_index > len(context):\n",
    "        return \"No answer found\"\n",
    "\n",
    "    answer = context[top_start_index:top_end_index]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a545fc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    }
   ],
   "source": [
    "context = \"The quick brown fox jumps over the lazy dog.\"\n",
    "question = \"What color is the fox?\"\n",
    "\n",
    "answer = answer_question(context, question, tokenizer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209fba8e",
   "metadata": {},
   "source": [
    "## Output\n",
    " I trained the model with small dataset. So it provieded the incorrect output.To improve the accuracy of the model, we could try using a larger dataset that contains more examples of questions and answers related to colors of animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd6cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
