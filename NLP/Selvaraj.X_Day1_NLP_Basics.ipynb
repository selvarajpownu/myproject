{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb5d863",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfc525",
   "metadata": {},
   "source": [
    "### 1. Purpose of Text Preprocessing in NLP:\n",
    "Text preprocessing in NLP is essential to clean and prepare textual data for analysis. \n",
    "It involves various techniques like removing irrelevant characters, converting text to lowercase, \n",
    "tokenization, stemming, lemmatization, and handling stop words. \n",
    "This process ensures that the data is in a suitable format for further analysis, enhancing the effectiveness \n",
    "of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e236243",
   "metadata": {},
   "source": [
    "### 2. Tokenization in NLP:\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, phrases, or sentences. In Python, the nltk library is commonly used for tokenization. Tokenization is significant in text processing because it forms the foundation for various NLP tasks such as text analysis, sentiment analysis, and language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.']\n",
      "\n",
      "No. of tokens 8\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Tokenization is an essential step in NLP.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "print()\n",
    "print('No. of tokens',len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe9cbe3",
   "metadata": {},
   "source": [
    "### 3. Differences between Stemming and Lemmatization:\n",
    "Stemming and lemmatization are techniques used to reduce words to their base or root form. Stemming is a simpler and faster process, but it may result in non-real words. Lemmatization, on the other hand, considers the context of the word and produces real words, making it a more accurate but computationally expensive process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab4b67d",
   "metadata": {},
   "source": [
    "### 4. Stop Words in Text Preprocessing:\n",
    "Stop words are common words like \"and,\" \"the,\" and \"is\" that are often removed during text preprocessing. They have little semantic meaning and can impact the efficiency of NLP tasks. Removing stop words helps reduce the dimensionality of the data and improves the accuracy of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff0984",
   "metadata": {},
   "source": [
    "### 5. Removing Punctuation in Text Preprocessing:\n",
    "Removing punctuation is crucial in text preprocessing as it eliminates unnecessary symbols that do not contribute to the meaning of the text. This step helps improve the efficiency of NLP tasks by ensuring that the analysis focuses on meaningful words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb5146",
   "metadata": {},
   "source": [
    "### 6. Importance of Lowercase Conversion:\n",
    "Converting text to lowercase is a common step in text preprocessing to ensure uniformity. It prevents the model from treating the same word in different cases as distinct, improving the accuracy of NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae50df",
   "metadata": {},
   "source": [
    "### 7. Vectorization in Text Data:\n",
    "Vectorization involves converting text data into numerical vectors that machine learning algorithms can understand. Techniques like CountVectorizer in Python help transform text into a matrix of word counts, facilitating the training of machine learning models on textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4156012b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.', 'This document is the second document.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b07bd1",
   "metadata": {},
   "source": [
    "### 8. Normalization in NLP:\n",
    "Normalization in NLP involves transforming text data to a standard form. Techniques include stemming, lemmatization, and removing accents. Normalization ensures consistency in the representation of words, improving the performance of NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d559e816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "word = \"running\"\n",
    "stemmed_word = stemmer.stem(word)\n",
    "stemmed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee23fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
